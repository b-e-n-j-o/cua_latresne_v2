# -*- coding: utf-8 -*-
"""
sub_orchestrator_cua.py ‚Äî Pipeline de g√©n√©ration des visualisations et du CUA
------------------------------------------------------
1Ô∏è‚É£ G√©n√®re la carte 2D Folium √† partir du WKT de l‚Äôunit√© fonci√®re
2Ô∏è‚É£ G√©n√®re la visualisation 3D Plotly √† partir du m√™me WKT
3Ô∏è‚É£ Upload sur Supabase Storage (bucket: visualisation)
4Ô∏è‚É£ Construit l‚ÄôURL publique encod√©e pour affichage sur Vercel (/maps?t=...)
5Ô∏è‚É£ Cr√©e un shortlink / QR dynamique
6Ô∏è‚É£ Lance la g√©n√©ration du CUA DOCX avec QR
7Ô∏è‚É£ Upload final des artifacts et insertion dans latresne.pipelines
------------------------------------------------------
"""

import os
import sys
import json
import base64
import logging
import gc
import secrets
import string
import tracemalloc
import shutil
from pathlib import Path
from dotenv import load_dotenv
try:
    import psutil
except ImportError:
    psutil = None
from supabase import create_client
from sqlalchemy import create_engine, text

from CUA.carte2d.carte2d_rendu import generer_carte_2d_depuis_wkt
from CUA.map_3d import exporter_visualisation_3d_plotly_from_wkt
from CUA.cua_builder import run_builder
from INTERSECTIONS.export_gpkg_intersections import export_gpkg_from_wkt


# ============================================================
# üîß CONFIGURATION
# ============================================================
load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SERVICE_KEY") or os.getenv("SUPABASE_SERVICE_ROLE_KEY")
SUPABASE_BUCKET = "visualisation"
KERELIA_BASE_URL = "https://kerelia.fr/maps"

# ‚úÖ Un seul client global
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(message)s",
    datefmt="%H:%M:%S"
)
logger = logging.getLogger("sub_orchestrator_cua")

logger.info("üîó Client Supabase initialis√© avec succ√®s.")
logger.info(f"üåç URL Supabase : {SUPABASE_URL}")

# ============================================================
# üîó UTILITAIRES
# ============================================================

def log_memory(step: str) -> float:
    """Log la RAM utilis√©e √† chaque √©tape (si psutil disponible)."""
    if psutil is None:
        return 0.0
    mem_mb = psutil.Process().memory_info().rss / 1024**2
    logger.info(f"üîπ [{step}] RAM: {mem_mb:.1f} MB")
    return mem_mb


def log_memory_detailed(step: str) -> None:
    """Log RAM process + tracemalloc pour debug fin."""
    if psutil:
        rss = psutil.Process().memory_info().rss / 1024**2
        logger.info(f"üîπ [{step}] RAM Process: {rss:.1f} MB")

    if tracemalloc.is_tracing():
        current, peak = tracemalloc.get_traced_memory()
        logger.info(
            f"   ‚îî‚îÄ Tracemalloc: current={current/1024**2:.1f}MB, "
            f"peak={peak/1024**2:.1f}MB"
        )


def generate_short_slug(length=26):
    """G√©n√®re un slug al√©atoire s√©curis√© (sans caract√®res ambigus O, 0, I, l)."""
    alphabet = ''.join(ch for ch in string.ascii_letters + string.digits if ch not in 'O0Il')
    return ''.join(secrets.choice(alphabet) for _ in range(length))


def upload_to_supabase(local_path, remote_path, bucket=None):
    """Upload d'un fichier vers Supabase Storage et renvoie l'URL publique.
    Lit le fichier, uploade, puis lib√®re le contenu pour limiter la RAM."""
    bucket = bucket or SUPABASE_BUCKET
    with open(local_path, "rb") as f:
        content = f.read()
    supabase.storage.from_(bucket).upload(
        remote_path,
        content,
        {
            "content-type": "application/octet-stream",
            "cache-control": "no-cache",
            "upsert": "true",
        },
    )
    del content
    return f"{SUPABASE_URL}/storage/v1/object/public/{bucket}/{remote_path}"


def compute_centroid_from_wkt_path(wkt_path: str):
    """
    Calcule le centro√Øde (lon/lat) de l'unit√© fonci√®re √† partir d'un fichier WKT,
    en s'appuyant sur PostGIS (m√™me base que pour les intersections).
    """
    try:
        wkt_file = Path(wkt_path)
        if not wkt_file.exists():
            logger.warning(f"‚ö†Ô∏è WKT introuvable pour calcul du centro√Øde: {wkt_path}")
            return None

        geom_wkt = wkt_file.read_text(encoding="utf-8").strip()
        if not geom_wkt:
            logger.warning("‚ö†Ô∏è WKT vide pour calcul du centro√Øde")
            return None

        SUPABASE_HOST = os.getenv("SUPABASE_HOST")
        SUPABASE_DB = os.getenv("SUPABASE_DB")
        SUPABASE_USER = os.getenv("SUPABASE_USER")
        SUPABASE_PASSWORD = os.getenv("SUPABASE_PASSWORD")
        SUPABASE_PORT = os.getenv("SUPABASE_PORT") or "5432"

        if not all([SUPABASE_HOST, SUPABASE_DB, SUPABASE_USER, SUPABASE_PASSWORD]):
            logger.warning("‚ö†Ô∏è Variables DB manquantes, impossible de calculer le centro√Øde")
            return None

        database_url = (
            f"postgresql+psycopg2://{SUPABASE_USER}:{SUPABASE_PASSWORD}"
            f"@{SUPABASE_HOST}:{SUPABASE_PORT}/{SUPABASE_DB}"
        )
        engine = create_engine(database_url)

        with engine.connect() as conn:
            lon, lat = conn.execute(
                text(
                    """
                    SELECT
                        ST_X(ST_Transform(ST_Centroid(ST_GeomFromText(:wkt, 2154)), 4326)) AS lon,
                        ST_Y(ST_Transform(ST_Centroid(ST_GeomFromText(:wkt, 2154)), 4326)) AS lat
                    """
                ),
                {"wkt": geom_wkt},
            ).one()

        if lon is None or lat is None:
            logger.warning("‚ö†Ô∏è Centro√Øde NULL retourn√© par PostGIS")
            return None

        centroid = {"lon": float(lon), "lat": float(lat)}
        logger.info(f"üìç Centro√Øde UF calcul√©: {centroid}")
        return centroid

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erreur lors du calcul du centro√Øde UF: {e}")
        return None


# ============================================================
# üß© PIPELINE PRINCIPAL
# ============================================================

def generer_visualisations_et_cua_depuis_wkt(
    wkt_path,
    out_dir,
    commune="latresne",
    code_insee="33234",
    skip_3d: bool = False,
    skip_gpkg: bool = False,
):
    OUT_DIR = out_dir
    os.makedirs(OUT_DIR, exist_ok=True)

    # ============================================================
    # üß™ LOGS M√âMOIRE D√âTAILL√âS DANS UN FICHIER D√âDI√â
    # ============================================================
    log_file = os.path.join(OUT_DIR, "memory_audit.log")
    file_handler = logging.FileHandler(log_file, mode="w", encoding="utf-8")
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(
        logging.Formatter("%(asctime)s | %(levelname)-8s | %(message)s")
    )
    # √âviter de dupliquer les FileHandler si la fonction est rappel√©e
    logger.handlers = [
        h for h in logger.handlers if not isinstance(h, logging.FileHandler)
    ]
    logger.addHandler(file_handler)
    logger.info(f"üß™ Memory audit log initialis√© : {log_file}")
    # Tracemalloc pour audit d√©taill√©
    if not tracemalloc.is_tracing():
        tracemalloc.start()
    log_memory_detailed("START")
    log_memory("CUA_START")

    # ============================================================
    # üîë G√âN√âRATION DU SLUG EN AMONT (source de v√©rit√©)
    # ============================================================
    slug = generate_short_slug()
    logger.info(f"üîë Slug g√©n√©r√© : {slug}")
    
    # ============================================================
    # ü©π V√âRIFICATION ROBUSTE DU FICHIER WKT
    # ============================================================
    wkt_path = Path(wkt_path)
    if not wkt_path.exists():
        raise FileNotFoundError(f"‚ùå Fichier WKT introuvable : {wkt_path}")
    logger.info(f"üöÄ Lancement du pipeline global pour le WKT : {wkt_path}")

    # --------------------------------------------------------
    # √âTAPE 1 : CARTE 2D (NOUVEAU MOTEUR)
    # --------------------------------------------------------
    logger.info("\nüì¶ √âTAPE 1/5 : G√©n√©ration de la carte 2D (nouvelle version)")

    html_2d, metadata_2d = generer_carte_2d_depuis_wkt(
        wkt_path=str(wkt_path),
        code_insee=code_insee,
        inclure_ppri=True,
        ppri_table=f"{commune}.pm1_detaillee_gironde"
    )
    log_memory_detailed("CARTE_2D_GENERATED")

    html_2d_path = os.path.join(OUT_DIR, "carte_2d_unite_fonciere.html")
    with open(html_2d_path, "w", encoding="utf-8") as f:
        f.write(html_2d)
    log_memory_detailed("CARTE_2D_WRITTEN")
    del html_2d
    gc.collect()
    log_memory_detailed("CARTE_2D_FREED")
    log_memory("APRES_CARTE_2D")
    logger.info(f"‚úÖ Carte 2D (nouvelle version) sauvegard√©e : {html_2d_path}")

    # --------------------------------------------------------
    # √âTAPE 2 : CARTE 3D
    # --------------------------------------------------------
    logger.info("\nüì¶ √âTAPE 2/5 : G√©n√©ration de la visualisation 3D Plotly")
    path_3d = None
    metadata_3d = None
    url_3d = None
    if not skip_3d:
        res3d = exporter_visualisation_3d_plotly_from_wkt(
            wkt_path=wkt_path,
            output_dir=OUT_DIR,
            exaggeration=1.5
        )
        log_memory_detailed("CARTE_3D_GENERATED")
        path_3d = res3d["path"]
        metadata_3d = res3d.get("metadata")
        del res3d
        gc.collect()
        log_memory("APRES_CARTE_3D")
        logger.info(f"‚úÖ Carte 3D g√©n√©r√©e : {path_3d}")
    else:
        logger.info("‚è≠Ô∏è Carte 3D d√©sactiv√©e (skip_3d=True)")

    # --------------------------------------------------------
    # √âTAPE 3 : UPLOAD SUPABASE (cartes)
    # --------------------------------------------------------
    logger.info("\nüì¶ √âTAPE 3/5 : Upload des cartes sur Supabase...")
    # ‚úÖ Utiliser le slug comme r√©pertoire distant unique
    remote_dir = slug
    remote_2d = f"{remote_dir}/carte_2d.html"
    remote_3d = f"{remote_dir}/carte_3d.html"

    url_2d = upload_to_supabase(html_2d_path, remote_2d)
    if path_3d and not skip_3d:
        url_3d = upload_to_supabase(path_3d, remote_3d)
    else:
        url_3d = None
    gc.collect()
    log_memory("APRES_UPLOADS")
    log_memory_detailed("UPLOADS_DONE")

    logger.info(f"üåê URL publique 2D : {url_2d}")
    logger.info(f"üåê URL publique 3D : {url_3d}")

    # --------------------------------------------------------
    # √âTAPE 4 : CONSTRUCTION DU LIEN ENCODED + SHORTLINK
    # --------------------------------------------------------
    logger.info("\nüì¶ √âTAPE 4/5 : Construction du lien encod√© et du QR dynamique")
    payload = {"carte2d": url_2d, "carte3d": url_3d, "commune": commune}
    token = base64.urlsafe_b64encode(json.dumps(payload).encode()).decode()
    maps_page_url = f"{KERELIA_BASE_URL}?t={token}"

    # ‚úÖ Utiliser le slug d√©j√† g√©n√©r√© (shortlinks dans public)
    logger.info(f"üß© Insertion du shortlink dans public.shortlinks (slug={slug})...")
    try:
        response = supabase.schema("public").table("shortlinks").upsert({
            "slug": slug,
            "target_url": maps_page_url
        }).execute()
        logger.info(f"‚úÖ Shortlink cr√©√© (status={getattr(response, 'status_code', '?')}) : https://kerelia.fr/m/{slug}")
        qr_url = f"https://kerelia.fr/m/{slug}"
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erreur lors de la cr√©ation du shortlink : {e}")
        qr_url = maps_page_url

    # ============================================================
    # üìÑ √âTAPE 5 : G√©n√©ration du CUA DOCX
    # ============================================================
    logger.info("\nüì¶ √âTAPE 5/5 : G√©n√©ration du CUA DOCX avec QR dynamique")

    BASE_DIR = os.path.dirname(__file__)

    cerfa_path = os.path.join(OUT_DIR, "cerfa_result.json")
    intersections_files = list(Path(OUT_DIR).glob("rapport_intersections_*.json"))
    if intersections_files:
        intersections_path = str(max(intersections_files, key=lambda p: p.stat().st_mtime))
        logger.info(f"üìë Rapport d'intersections utilis√© : {intersections_path}")
    else:
        raise FileNotFoundError(f"‚ùå Aucun rapport d'intersections trouv√© dans {OUT_DIR}")

    # ------------------------------------------------------------
    # üì§ Upload du JSON d'intersections sur Supabase
    # ------------------------------------------------------------
    logger.info("üì§ Upload du JSON d'intersections...")
    
    intersections_filename = Path(intersections_path).name
    remote_intersections_json = f"{remote_dir}/{intersections_filename}"
    
    intersections_json_url = upload_to_supabase(
        intersections_path,
        remote_intersections_json,
        bucket=SUPABASE_BUCKET
    )
    
    logger.info(f"üåê URL publique JSON intersections : {intersections_json_url}")

    # R√©cup√©ration des m√©tadonn√©es CERFA
    cerfa_data = None
    cerfa_data_json = os.getenv("CERFA_DATA_JSON")
    if cerfa_data_json:
        try:
            cerfa_data = json.loads(cerfa_data_json)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur lors du parsing de CERFA_DATA_JSON : {e}")

    # ============================================================
    # üì¶ Export GPKG intersections
    # ============================================================
    logger.info("\nüì¶ Export GPKG intersections...")
    gpkg_path = os.path.join(OUT_DIR, "intersections.gpkg")

    intersections_gpkg_url = None
    if not skip_gpkg:
        export_gpkg_from_wkt(str(wkt_path), gpkg_path)
        log_memory_detailed("GPKG_GENERATED")
        gc.collect()
        log_memory("APRES_GPKG")

        remote_gpkg = f"{remote_dir}/intersections.gpkg"
        intersections_gpkg_url = upload_to_supabase(gpkg_path, remote_gpkg)
        logger.info(f"üåê URL publique GPKG : {intersections_gpkg_url}")
    else:
        logger.info("‚è≠Ô∏è Export GPKG d√©sactiv√© (skip_gpkg=True)")

    # Utiliser le catalogue avec geom_type
    catalogue_path = os.path.join(BASE_DIR, "..", "catalogues", "catalogue_intersections_tagged.json")
    output_docx_path = os.path.join(OUT_DIR, "CUA_unite_fonciere.docx")
    logo_latresne_path = os.path.join(BASE_DIR, "logos", "logo_latresne.png")
    logo_kerelia_path = os.path.join(BASE_DIR, "logos", "logo_kerelia.png")

    try:
        run_builder(
            cerfa_json=cerfa_path,
            intersections_json=intersections_path,
            catalogue_json=catalogue_path,
            output_path=output_docx_path,
            wkt_path=str(wkt_path),
            logo_first_page=logo_latresne_path,
            signature_logo=logo_kerelia_path,
            qr_url=qr_url,
            plu_nom="PLU de Latresne",
            plu_date_appro="13/02/2017"
        )
        logger.info("‚úÖ CUA DOCX g√©n√©r√© avec succ√®s.")
    except Exception as e:
        logger.error(f"üí• √âchec g√©n√©ration CUA DOCX : {e}")
        raise
    gc.collect()
    log_memory_detailed("DOCX_GENERATED")
    log_memory("APRES_DOCX")

    # ============================================================
    # üì§ UPLOAD FINAL DES ARTIFACTS (CUA uniquement)
    # ============================================================
    logger.info("\nüì§ Upload final du CUA vers Supabase...")

    # Upload du CUA dans le bucket visualisation
    remote_cua = f"{remote_dir}/CUA_unite_fonciere.docx"
    cua_url = upload_to_supabase(output_docx_path, remote_cua, bucket=SUPABASE_BUCKET)
    logger.info(f"üìé CUA upload√© dans {SUPABASE_BUCKET} : {cua_url}")
    gc.collect()
    log_memory("APRES_UPLOAD_FINAL")
    log_memory_detailed("UPLOAD_FINAL_DONE")

    # ============================================================
    # üîë G√âN√âRATION DU TOKEN POUR L'URL /cua?t={token}
    # ============================================================
    logger.info("\nüîë G√©n√©ration du token pour l'URL /cua...")
    payload_cua = {
        "docx": remote_cua  # Chemin relatif dans le bucket visualisation
    }
    token_cua = base64.b64encode(json.dumps(payload_cua).encode()).decode()
    cua_viewer_url = f"https://kerelia.fr/cua?t={token_cua}"
    logger.info(f"‚úÖ URL CUA g√©n√©r√©e : {cua_viewer_url}")

    # üß† L'upload du pipeline_result.json est d√©sormais g√©r√© par orchestrator_global.py
    result_url = None

    # ============================================================
    # üìã R√âSULTAT UNIFI√â (pour l'API et le front)
    # ============================================================
    result = {
        "slug": slug,
        "commune": commune,
        "code_insee": code_insee,
        "maps_page": maps_page_url,
        "qr_url": qr_url,
        "carte_2d_url": url_2d,
        "carte_3d_url": url_3d,
        "intersections_gpkg_url": intersections_gpkg_url,
        "intersections_json": intersections_json_url,
        "output_cua": cua_url,
        "cua_viewer_url": cua_viewer_url,  # URL pour afficher le CUA en HTML
        "bucket_path": remote_dir,
        "pipeline_result_url": result_url,
        "metadata_2d": metadata_2d,
        "metadata_3d": metadata_3d,
        "cerfa_data": cerfa_data,  # M√©tadonn√©es CERFA
        "status": "success",
    }

    # ============================================================
    # üíæ √âCRITURE DU FICHIER R√âSULTAT (lisible par l'API)
    # ============================================================
    sub_result_path = Path(OUT_DIR) / "sub_orchestrator_result.json"
    sub_result_path.write_text(json.dumps(result, indent=2, ensure_ascii=False), encoding="utf-8")
    logger.info(f"üíæ R√©sultat √©crit dans : {sub_result_path}")

    # ============================================================
    # üóÑÔ∏è ENREGISTREMENT EN BASE (facultatif mais recommand√©)
    # ============================================================
    # üßë‚Äçüíº R√©cup√©ration des infos utilisateur depuis l'environnement
    user_id = os.getenv("USER_ID") or None
    user_email = os.getenv("USER_EMAIL") or None

    # üìå Extraction des parcelles pour historisation (depuis cerfa_data si disponible)
    parcelles_for_history = None
    if cerfa_data:
        parcelles_for_history = (
            cerfa_data.get("references_cadastrales")
            or cerfa_data.get("parcelles")
            or None
        )

    # üìç Calcul du centro√Øde de l'unit√© fonci√®re pour l'historique carto
    centroid_for_history = compute_centroid_from_wkt_path(str(wkt_path))
    
    logger.info(f"üß© Insertion pipeline dans latresne.pipelines (slug={slug})...")
    try:
        response = supabase.schema("latresne").table("pipelines").upsert({
            "slug": slug,
            "code_insee": code_insee,
            "commune": commune,
            "status": "success",
            "bucket_path": remote_dir,
            "output_cua": cua_url,
            "carte_2d_url": url_2d,
            "carte_3d_url": url_3d,
            "qr_url": qr_url,
            "pipeline_result_url": result_url,
            "user_id": user_id,
            "user_email": user_email,
            "cerfa_data": cerfa_data,
            "parcelles": parcelles_for_history,
            "centroid": centroid_for_history,
            "intersections_gpkg_url": intersections_gpkg_url,
            "intersections_json_url": intersections_json_url,
            "metadata": result,
            "suivi": 2,  # Dossier trait√© (√©tapes 1 et 2 valid√©es automatiquement)
        }).execute()
        logger.info(f"‚úÖ Pipeline enregistr√© dans latresne.pipelines (status={getattr(response, 'status_code', '?')})")
        if user_id:
            logger.info(f"üë§ Pipeline associ√© √† l'utilisateur : {user_email or user_id}")
    except Exception as e:
        logger.error(f"üí• Erreur d'insertion dans latresne.pipelines : {e}")

    # --------------------------------------------------------
    # FIN DU PIPELINE
    # --------------------------------------------------------
    logger.info("\nüéâ PIPELINE CUA TERMIN√â AVEC SUCC√àS üéâ")
    logger.info(f"üì¶ Slug unique : {slug}")
    logger.info(f"üîó Lien court : {qr_url}")

    # Copie du fichier de logs m√©moire dans /tmp pour Render / debug externe
    try:
        shutil.copy(log_file, "/tmp/memory_audit.log")
        logger.info("üìÅ memory_audit.log copi√© vers /tmp/memory_audit.log")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Impossible de copier memory_audit.log vers /tmp : {e}")

    return result


# ============================================================
# ‚ñ∂Ô∏è CLI
# ============================================================
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Sous-orchestrateur pour g√©n√©ration carte 2D, 3D et CUA.")
    parser.add_argument("--wkt", required=True, help="Chemin vers le fichier WKT (unit√© fonci√®re)")
    parser.add_argument("--out-dir", required=True, help="Dossier de sortie timestamp√©")
    parser.add_argument("--code_insee", default="33234")
    parser.add_argument("--commune", default="latresne")
    args = parser.parse_args()

    result = generer_visualisations_et_cua_depuis_wkt(
        wkt_path=args.wkt,
        out_dir=args.out_dir,
        commune=args.commune,
        code_insee=args.code_insee
    )

    print("\nüì¶ R√âSULTAT FINAL :")
    print(json.dumps(result, indent=2, ensure_ascii=False))
